---
title: "SVEPM Workshop 2022"
output:
  html_document:
    df_print: paged
---
```{r  global_options, echo=FALSE}
knitr::opts_chunk$set(error = FALSE, message = FALSE,
warning = FALSE)
```

## Introduction

The following packages are required for these excersises. 

```{r}
library(Hmisc)
library(broom)
library(glmnet)
library(ncvreg)
library(bigstep)
library(rsample)
library(tidyverse)
library(stabiliser)
```

## Simulating data

In order to appreciate the issues we will be discussing today, we have provided functions to simulate datasets for exploration. 

The following function generates a dataset with "ncols" as the number of variables and "nrows" as the number of rows. 

```{r}
generate_uncor_variables <- function(ncols, nrows){
  df <- data.frame(replicate(ncols, rnorm(nrows, 0, 1)))
  df
}
```

A dataset with 197 rows and 130 variables can be generated as follows: 

```{r}
variables <- generate_uncor_variables(ncols=130, nrows=197)
```

This results in the following dataset being generated:

```{r}
variables
```

We can also generate an outcome variable, in this case randomly generated in the same manner, but renaming as "outcome"

```{r}
generate_uncor_outcome <- function(nrows, seed=224){
  set.seed(seed)
  data.frame(replicate(1, rnorm(nrows, 0, 1))) %>%
    rename("outcome" = 1)
}

outcome <- generate_uncor_outcome(nrows=197)

outcome
```

We can now bind together the uncorrelated, randomly generated variables, with the randomly generated outcome. 

```{r}
df <- outcome %>%
  bind_cols(variables)
```

This results in a dataset as follows: 

```{r}
df
```

## Conventional approaches

We will test a variety of models on this dataset. For future comparison let's set up a list where we can store model results

```{r}
model_results <- list()
```


The following function conducts univariable analysis to determine the association between a given variable and the outcome. 

SPEARMAN/PEARSON/UNIVARIABLE MODEL available

```{r}
univariable_analysis <- function(data, variable){
  data %>%
    lm(outcome ~ variable, .) %>%
    tidy() %>%
    filter(term != "(Intercept)")
}
```

EXAMPLE OF ONE VARIABLE


This function can then be applied using map_df() to each column of the dataset individually and return a dataframe. 

```{r}
univariable_outcomes <- map_df(df, ~univariable_analysis(data=df, variable=.), .id="variable")
```

A conventional approach would then filter at a given threshold (for example P<0.2) 

```{r}
univariable_outcomes_filtered <- univariable_outcomes %>%
  filter(p.value < 0.2)

univariable_outcomes_filtered
```

A list of variables to be included is as follows: 

```{r}
variables_for_stepwise <- univariable_outcomes_filtered %>%
  pull(variable)
```

These variables would subsequently be offered into a stepwise selection process such as the following

```{r}
stepwise_model <- function(data, variables){
  data_selected <- data %>%
    select(variables)
  
  lm(outcome ~ ., data=data_selected) %>%
    step(., direction="backward", trace = FALSE) %>%
    tidy() %>%
    filter(p.value < 0.05) %>%
    rename(variable = term)
}

model_results$prefiltration <- stepwise_model(data = df, variables = variables_for_stepwise)

model_results$prefiltration
```

## Data with a true signal 

```{r}
generate_data_with_signal <- function(nrow, ncol, n_causal_vars, amplitude){
  set.seed(59719)
# Generate the variables from a multivariate normal distribution
mu = rep(0,ncol)
rho = 0.25
sigma = toeplitz(rho^(0:(ncol-1))) #  Symmetric Toeplitz Matrix
X = matrix(rnorm(nrow*ncol),nrow) %*% chol(sigma) # multiply matrices Choleski Decomposition. Description. Compute the Choleski factorization of a real symmetric positive-definite square matrix)

# Generate the response from a linear model
nonzero = sample(ncol, n_causal_vars) # select the id of 'true' variables
beta = amplitude * (1:ncol %in% nonzero) / sqrt(nrow) # vector of effect sizes to pick out true varaiables
beta_value =  amplitude / sqrt(nrow)
outcome.sample = function(X) X %*% beta + rnorm(nrow) # calculate outcome from true vars and error
outcome = outcome.sample(X)

## Rename true variables
X_data <- as.data.frame(X)
for(i in c(nonzero)){
  X_data1 <- X_data %>% 
    rename_with(.cols = i, ~paste("causal_",i,sep=""))
  X_data<-X_data1
}

dataset_sim <- as.data.frame(cbind(outcome, X_data1))
}

```

Plot signal strengths

True "cheat" model

```{r}
df <- generate_data_with_signal(nrow=300, ncol=300, n_causal_vars = 8, amplitude = 7)
univariable_outcomes <- map_df(df, ~univariable_analysis(data=df, variable=.), .id="variable")
univariable_outcomes_filtered <- univariable_outcomes %>%
  filter(p.value < 0.2)
variables_for_stepwise <- univariable_outcomes_filtered %>%
  pull(variable)
stepwise_model(data = df, variables = variables_for_stepwise)
```

## Regularisation

```{r}
model_lasso <- function(data) {
  y_temp <- data %>%
    select("outcome") %>%
    as.matrix()

  x_temp <- data %>%
    select(-"outcome") %>%
    as.matrix()

  fit_lasso <- cv.glmnet(x = x_temp, y = y_temp, alpha=1)

  coefs <- coef(fit_lasso, s = "lambda.min")

  data.frame(name = coefs@Dimnames[[1]][coefs@i + 1], coefficient = coefs@x) %>%
    rename(
      variable = name,
      estimate = coefficient
    ) %>%
    filter(variable != "(Intercept)") %>%
    select(variable, estimate)
}

model_results$lasso <- model_lasso(df)
```

MCP can also be used 

```{r}
model_mcp <- function(data) {
  y_temp <- data %>%
    select("outcome") %>%
    as.matrix()

  x_temp <- data %>%
    select(-"outcome")

  fit_mcp <- cv.ncvreg(X = x_temp, y = y_temp)

  fit_mcp %>%
    coef() %>%
    as_tibble(rownames = "variable") %>%
    rename(
      estimate = value
    ) %>%
    filter(
      variable != "(Intercept)",
      estimate != 0,
      !grepl("(Intercept)", variable),
      !grepl("Xm[, -1]", variable)
    ) %>%
    mutate(variable = str_remove_all(variable, "`"))
}

model_results$mcp <- model_mcp(df)
```


MBIC can also be used

```{r}
model_mbic <- function(data) {
  
  y_temp <- data %>%
    select("outcome") %>%
    as.matrix()

  x_temp <- data %>%
    select(-"outcome")

  bigstep_prepped <- bigstep::prepare_data(y_temp, x_temp, verbose = FALSE)

  bigstep_prepped %>%
    reduce_matrix(minpv = 0.01) %>%
    fast_forward(crit = mbic) %>%
    multi_backward(crit = mbic) %>%
    summary() %>%
    stats::coef() %>%
    as.data.frame() %>%
    rownames_to_column(., var = "variable") %>%
    mutate(variable = str_remove_all(variable, "`")) %>%
    filter(
      !grepl("(Intercept)", variable),
      !grepl("Xm[, -1]", variable)
    ) %>%
    rename(estimate = Estimate) %>%
    select(variable, estimate)
  
}

model_results$mbic <- model_mbic(df)
```

A comparison of the number of True/False positives is shown below:

```{r}
calculate_tp_fp <- function(results){
  results %>%
    mutate(causal = case_when(grepl("causal", variable) ~ "tp",
                            !grepl("causal", variable) ~ "fp")) %>%
    group_by(model) %>%
    summarise(tp = sum(causal == "tp", na.rm=TRUE),
              fp = sum(causal == "fp", na.rm=TRUE)) %>%
    mutate("total_selected" = tp + fp)
}

conventional_results <- model_results %>%
  bind_rows(., .id = "model") %>%
  calculate_tp_fp()
```

## Stability selection

Function for bootstrapping

```{r}
boot_sample <- function(data, boot_reps) {
  rsample::bootstraps(data, boot_reps)
}

bootstrapped_datasets <- boot_sample(data = df, boot_reps = 10)
```

Bootstrapped data is presented here as a table of 10 different nested tables. 

```{r}
bootstrapped_datasets
```

If we extract a single bootstrapped dataset and sort by the outcome, we can see that several rows have been resampled. Consequently as the dataset length is the same as the original, several rows will be omitted completely. 

```{r}
bootstrapped_datasets$splits[[1]] %>%
  as_tibble() %>%
  arrange(outcome)
```

## Model for bootstraps (lasso)

We can apply our previous lasso function over each one of these bootstrapped resamples. 

```{r}
model_lasso_bootstrapped <- bootstrapped_datasets %>%
    map_df(.x = .$splits, .f = ~ as.data.frame(.) %>% model_lasso(.), .id = "bootstrap")
```

## Permutation

To identify a null threshold, first we must permute the outcome. 

Our original dataset looks like this: 

```{r}
df
```

By permuting the outcome variable we sever all ties between the outcome and the explanatory variables. We might want to conduct this 5 times. 

```{r}
permuted_datasets <- rsample::permutations(data = df, permute = outcome, times = 5)
```

A single dataset would look like this. Note the structure of the explanatory variables remains the same, but the outcome is randomly shuffled. 

```{r}
permuted_datasets$splits[[1]] %>%
  as_tibble()
```

We can then apply our bootstrap function to each one of these 5 permuted datasets. We might perform 10 bootstrap samples for each of the 5 permuted datasets. 

```{r}
permuted_bootstrapped_datasets <- permuted_datasets %>%
    map_df(.x = .$splits, .f = ~ as.data.frame(.) %>% boot_sample(., boot_reps = 10), .id = "permutation")
```

```{r}
perm_model <- function(perm_data, data) {
  perm_data %>%
    mutate(perm_coefs = map(.x = .$splits, .f = ~ as.data.frame(.) %>%
      model_lasso(.), .id = "bootstrap")) %>%
    select(-splits) %>%
    unnest(perm_coefs) %>%
    select(-id) %>%
    group_by(permutation) %>%
    nest() %>%
    rename(perm_data = data) %>%
    map_df(.x = .$perm_data, .f = ~ as.data.frame(.) %>%
      boot_summarise(booted_obj = ., data = data, boot_reps = 10), .id = "permutation")
}

permed_models <- perm_model(perm_data = permuted_bootstrapped_datasets, 
                            data = df)
```



Draw the line

## stabiliser approach

```{r}
stab_output <- stabilise(outcome = "outcome", data = df, models = c("mbic"), type = "linear")

stab_output$mbic$stability
```

All models 

```{r}
stab_output <- stabilise(outcome = "outcome", data = df, models = c("mbic", "lasso", "mcp"), type = "linear")
```

# Plot differences between stab and non-stab

Calculate the number of true and false positives selected through stability approaches 

```{r}
stability_results <- stab_output %>%
  map_df(., ~ .x$stability, .id = "model") %>%
  filter(stable == "*") %>%
  calculate_tp_fp(.) %>%
  rename_all(., ~paste0(., "_stability"))

stability_results
```

Compare this with the non-stability approach

```{r}
conventional_results %>%
  left_join(stability_results, by=c("model" = "model_stability"))
```

# Triangulation

```{r}
triangulate(stab_output)
```



